
m4_changequote(`[[[',`]]]')

<style>
.pagebreak { page-break-before: always; }
.half { height: 200px; }
</style>

# Lecture 26 - What goes wrong..., "artificial stupidity"

<!--
 "Five Principles of AI Weirdness", including "AIs don't understand
 the problems you want them to solve" and "AIs take the path of least
 resistance to their programmed goal".[1] Shane gives many examples of AI
 "shortcuts", including the (possibly apocryphal) legend of an AI that
 appeared to reliably recognize tanks from photos, by noticing whether
 the photos were taken on a sunny or a cloudy day. Another of Shane's
 examples is a hypothetical scenario where a simulated AI evolved to
 keep people from entering a hazardous hallway during a fire emergency,
 learns the optimal strategy is to just kill everyone so they cannot enter
 the hallway. Because AI lacks general intelligence, Shane is skeptical
 of efforts to power self-driving cars or to detect online hate speech
 using artificial intelligence. Shane also pushes back against concerns
 artificial intelligence will replace people's jobs.[3]
-->

1/2 of this lecture is from,  "You Look Like a Thing and I Love You" by
Janelle Shane an AI Researcher, Janelle Shane.

## Making Flavors

![These-flavors-are-not-delicious.png](These-flavors-are-not-delicious.png)

## Walker goes wacky

You can train a NN to walk!

Principals:
1. A.I. takes the path of least resistance.
2. A.I. dont understand the problem you are trying to solve.
3. The danger of AI is not that it's too smart but it's not smart enough.


## Police and Racism

[https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/](https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/)

## The "Algorithm"

[https://www.nytimes.com/2019/08/11/world/americas/youtube-brazil.html](https://www.nytimes.com/2019/08/11/world/americas/youtube-brazil.html)


